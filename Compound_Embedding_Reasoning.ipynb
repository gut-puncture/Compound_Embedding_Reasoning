{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxtxoZNDHv37MgwsacIA0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7af3a4da1c69466a83a9a4426dcaa99c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8faead9bb2f4f05aec1cb419192ada0",
              "IPY_MODEL_92b3a30b2e1f4cfcbccc44dd2558e37c",
              "IPY_MODEL_84693eb680f4434ba17661952e67a290"
            ],
            "layout": "IPY_MODEL_6c119b6a177048b8b052732cae20b2aa"
          }
        },
        "e8faead9bb2f4f05aec1cb419192ada0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5514a7e81d4944aba7a51886fc236e73",
            "placeholder": "​",
            "style": "IPY_MODEL_1c12343385264e6f811776b61cbc096b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "92b3a30b2e1f4cfcbccc44dd2558e37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a579d3d486724cb3ad0dbc85c3b6edae",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ea4efc0c5a04f2a8d4ea21c843ad7d4",
            "value": 2
          }
        },
        "84693eb680f4434ba17661952e67a290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dc923177c80483b9a60364fdffac284",
            "placeholder": "​",
            "style": "IPY_MODEL_6cade08d333e49ffb6f148d9e5399e12",
            "value": " 2/2 [02:17&lt;00:00, 65.75s/it]"
          }
        },
        "6c119b6a177048b8b052732cae20b2aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5514a7e81d4944aba7a51886fc236e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c12343385264e6f811776b61cbc096b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a579d3d486724cb3ad0dbc85c3b6edae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ea4efc0c5a04f2a8d4ea21c843ad7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1dc923177c80483b9a60364fdffac284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cade08d333e49ffb6f148d9e5399e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gut-puncture/Compound_Embedding_Reasoning/blob/main/Compound_Embedding_Reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Y8s_rV8ElvbE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beIuPWYPBST_",
        "outputId": "e7c31678-603f-4300-bc1f-954a4638413f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1️⃣ Mount your Drive so Colab sees it as a local folder.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 2️⃣ Define where you want to store the model weights *permanently*.\n",
        "MODEL_DIR = \"/content/drive/MyDrive/phi3_3.8B\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Install the libraries we'll need.\n",
        "!pip install --upgrade \"transformers==4.41.2\" \"huggingface_hub>=0.23.0\" \"accelerate>=0.29.0\" sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "t63tE2sUE0Rs",
        "outputId": "5c5dd266-bd8e-421e-a7f8-babed85ccb75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Collecting huggingface_hub>=0.23.0\n",
            "  Downloading huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: accelerate>=0.29.0 in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0) (4.13.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.23.0)\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.29.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.29.0) (3.0.2)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.32.2-py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-xet, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, tokenizers, nvidia-cusolver-cu12, transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.31.4\n",
            "    Uninstalling huggingface-hub-0.31.4:\n",
            "      Successfully uninstalled huggingface-hub-0.31.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "Successfully installed hf-xet-1.1.2 huggingface_hub-0.32.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.19.1 transformers-4.41.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "torch.set_printoptions(precision=16, sci_mode=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    torch_dtype=\"auto\",                 # Uses float16 on GPU, float32 on CPU.\n",
        "    device_map=\"auto\"                   # transformers + accelerate decide the best device.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "7af3a4da1c69466a83a9a4426dcaa99c",
            "e8faead9bb2f4f05aec1cb419192ada0",
            "92b3a30b2e1f4cfcbccc44dd2558e37c",
            "84693eb680f4434ba17661952e67a290",
            "6c119b6a177048b8b052732cae20b2aa",
            "5514a7e81d4944aba7a51886fc236e73",
            "1c12343385264e6f811776b61cbc096b",
            "a579d3d486724cb3ad0dbc85c3b6edae",
            "0ea4efc0c5a04f2a8d4ea21c843ad7d4",
            "1dc923177c80483b9a60364fdffac284",
            "6cade08d333e49ffb6f148d9e5399e12"
          ]
        },
        "id": "ZuIh5qD0E6Gp",
        "outputId": "98685d0b-0adf-4117-f24d-472a014d5c00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7af3a4da1c69466a83a9a4426dcaa99c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "pwHgAhE_lyoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_text = \"What is Photosynthesis?\" #will be populated by the eval questions"
      ],
      "metadata": {
        "id": "HsiYqyYTR8GX"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_start_tokens = \"### Reasoning:\\n\"\n",
        "reasoning_end_tokens = \"###\"\n",
        "answer_start_tokens = \"### Answer:\\n\"\n",
        "sys_prompt = \"You are a helpful assistant to a human. You will think deeply about any user request and asnwer as smartly as possible.\"\n",
        "prompt = (\n",
        "  f\"<|system|>\\n{sys_prompt}<|end|>\\n\"\n",
        "  f\"<|user|>\\n{user_text}<|end|>\\n\"\n",
        "  f\"<|assistant|>\\n### Reasoning:\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "5hXbNZitQgbd"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generating next tokens\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mhhzZhIbRw4t"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_logits, sorted_indices = torch.sort(outputs.logits[:,-1,:], descending=True) #sorting the logits so we can do top-p sampling\n",
        "sorted_probs = torch.softmax(sorted_logits, dim=-1) #converted sorted logits into sorted probs\n",
        "cumulative_probs = torch.cumsum(sorted_probs, dim=-1) #doing a cumulative sum of probs so we can identify when the top-p sampling cut-off is reached"
      ],
      "metadata": {
        "id": "mRKQcu2NSHd5"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling only those tokens which have a combined probs of p\n",
        "p_compound_vector = 0.98\n",
        "selected_token_indices = []\n",
        "\n",
        "for token in range(len(sorted_indices.tolist()[0])):\n",
        "  if cumulative_probs.tolist()[0][token] < p_compound_vector:\n",
        "    selected_token_indices.append(sorted_indices.tolist()[0][token]) #token indices are actually token ids as well\n",
        "  else:\n",
        "    break\n",
        "print(selected_token_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7JaHHvZc4EA",
        "outputId": "df9a794d-90eb-4055-892a-b9504f434c5f"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13, 1762, 29896, 29899, 1576, 4819, 29902, 797]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_token_probs = sorted_probs[:,:len(selected_token_indices)].tolist()[0] #selecting the token probs for the selected token ids\n",
        "selected_token_logits = sorted_logits[:,:len(selected_token_indices)].tolist()[0] #selecting the token logits for the selected token ids"
      ],
      "metadata": {
        "id": "5GhCYVUGUWSq"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting embeddings of the selected tokens\n",
        "\n",
        "embeddings = model.model.embed_tokens #method to get token embeddings\n",
        "selected_token_indices_tensor = torch.tensor(selected_token_indices, dtype=torch.long).to('cuda') #converted list to tensor\n",
        "selected_token_embeddings = embeddings(selected_token_indices_tensor)"
      ],
      "metadata": {
        "id": "G7zuOzkhr1Z8"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#renormalising probs\n",
        "selected_token_renormalised_probs = torch.softmax(torch.tensor(selected_token_logits, dtype=torch.float32), dim=-1).to('cuda')\n",
        "selected_token_embeddings = selected_token_embeddings.to(torch.float32) #bringing both to same precision\n",
        "\n",
        "compound_embedding_vector = selected_token_embeddings * (selected_token_renormalised_probs).unsqueeze(-1).to('cuda')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "utXLKRmOAlvb"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compound_embedding_vector_summed = torch.sum(compound_embedding_vector, dim=0)\n",
        "compound_embedding_vector_summed = compound_embedding_vector_summed.unsqueeze(0).unsqueeze(0)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mjQEzJvY_n8q"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#sampling probability is different from the p for compound vector.\n",
        "#0.8 is a good value for top-p sampling and a higher value could lead to incoherent generation.\n",
        "p = 0.80\n",
        "sorted_probs_sampling = torch.softmax(outputs.logits[:, -1, :], dim=-1)\n",
        "sorted_probs_sampling, sorted_indices_sampling = torch.sort(sorted_probs_sampling, descending=True)\n",
        "cumulative_probs_sampling = torch.cumsum(sorted_probs_sampling, dim=-1)\n",
        "\n",
        "# Find the indices where cumulative probability is less than p\n",
        "# Adding a small epsilon to cumulative_probs to handle floating point inaccuracies and include the token that makes the cumulative sum >= p\n",
        "indices_to_remove_sampling = cumulative_probs_sampling > p\n",
        "# Keep at least one token\n",
        "indices_to_remove_sampling[..., 0] = False\n",
        "\n",
        "# Set the probability of the tokens to be removed to zero\n",
        "sorted_probs_sampling[indices_to_remove_sampling] = 0\n",
        "\n",
        "# Renormalize the remaining probabilities\n",
        "sorted_probs_sampling /= sorted_probs_sampling.sum(dim=-1, keepdim=True)\n",
        "\n",
        "# Sample a token from the remaining probabilities\n",
        "sampled_token_index = torch.multinomial(sorted_probs_sampling, num_samples=1)\n",
        "sampled_token_id = sorted_indices_sampling[0, sampled_token_index]\n",
        "\n",
        "print(f\"Sampled Token ID: {sampled_token_id.item()}\")\n",
        "sampled_token = tokenizer.decode(sampled_token_id.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v0k15ZG5cfrw",
        "outputId": "a3b4788a-48dd-4650-dc8c-16a184a48362"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled Token ID: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compound_embedding_vector_summed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMNZIMCMmbEE",
        "outputId": "5027c55e-37d8-4ae4-8e06-30956a4cf26b"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0055628521367908, -0.0006621304783039,  0.0024661654606462,\n",
              "           ...,  0.0114772515371442,  0.0603033900260925,\n",
              "          -0.0042509892955422]]], device='cuda:0',\n",
              "       grad_fn=<UnsqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.25\n",
        "\n",
        "# introducing compound vector into model\n",
        "sampled_token_embedding_top_p = embeddings(sampled_token_id)\n",
        "\n",
        "#summing sample token embedding with compound vector\n",
        "thinking_advance_vector_embedding = (1-alpha)*sampled_token_id + alpha*compound_embedding_vector_summed"
      ],
      "metadata": {
        "id": "TK-Gsf8_4D4f"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZaEZLGBlYZH"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thinking_advance_vector_embedding = thinking_advance_vector_embedding.to(model.dtype) #ensuring correct datatype"
      ],
      "metadata": {
        "id": "I32UwdpY_VQk"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_embeddings = embeddings(inputs) #the embedding vectors for each token in the prompt\n",
        "\n",
        "#our thinking advancement vector added after the prompt vectors as if our vector corresponds to the next token\n",
        "combined_embeddings = torch.cat((prompt_embeddings, thinking_advance_vector_embedding), dim=1)"
      ],
      "metadata": {
        "id": "PrP53Dz7ZIUM"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_attention_mask = torch.ones(inputs.shape, dtype=torch.long).to('cuda')\n",
        "\n",
        "# Extend the attention mask by adding a column of ones for your vector's position\n",
        "new_column_mask = torch.ones((original_attention_mask.shape[0], 1), dtype=torch.long).to('cuda')"
      ],
      "metadata": {
        "id": "WsyanFc7olos"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_attention_mask = torch.cat((original_attention_mask, new_column_mask), dim=1)"
      ],
      "metadata": {
        "id": "xmdf7AEXokNZ"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_attention_mask.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djK4OTjPpLZ8",
        "outputId": "8d8f7b8a-a4d4-4b5c-b5d0-fc8b3eaf583a"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seq_length = combined_attention_mask.shape[1]\n",
        "causal_mask = torch.tril(torch.ones((seq_length, seq_length), dtype=torch.bool, device=combined_attention_mask.device))\n",
        "\n",
        "expanded_padding_mask = combined_attention_mask.bool().unsqueeze(1).unsqueeze(2).expand(-1, 1, seq_length, -1)\n",
        "\n",
        "final_attention_mask = expanded_padding_mask & causal_mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "attention_mask_float = torch.where(\n",
        "    final_attention_mask,\n",
        "    torch.zeros(1, dtype=hidden_states.dtype, device=hidden_states.device),\n",
        "    torch.full([], torch.finfo(hidden_states.dtype).min, device=hidden_states.device),\n",
        ")\n",
        "\n",
        "\n",
        "# Ensure position_ids matches the current sequence length\n",
        "current_sequence_length = hidden_states.shape[1]\n",
        "position_ids = torch.arange(0, current_sequence_length, dtype=torch.long, device=hidden_states.device).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "285-bmEDupaq"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_layers = model.model.layers\n",
        "hidden_states = combined_embeddings"
      ],
      "metadata": {
        "id": "KWOUc5dO0MDO"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in transformer_layers:\n",
        "    reshaped_attention_mask = combined_attention_mask.unsqueeze(1).unsqueeze(1)\n",
        "    layer_output = layer(\n",
        "        hidden_states,\n",
        "        attention_mask=attention_mask_float,\n",
        "        position_ids=position_ids)\n",
        "    hidden_states = layer_output[0]"
      ],
      "metadata": {
        "id": "TCb5Eui_pfL1"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vector_output_hidden_state = hidden_states[:, -1:, :]\n",
        "# shape: [batch_size, 1, hidden_size]\n",
        "\n",
        "# Pass this hidden state through the language model head to get logits.\n",
        "# The language model head is often `model.lm_head`.\n",
        "logits_for_vector_position = model.lm_head(vector_output_hidden_state)"
      ],
      "metadata": {
        "id": "DqAd9_zhpIrO"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_for_vector_position"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Se1_og0zAy",
        "outputId": "918a25a0-b414-4aaf-b62d-d2c1e49747f1"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[159.0000000000000000, 228.0000000000000000, 149.0000000000000000,\n",
              "           ...,  69.0000000000000000,  68.5000000000000000,\n",
              "           68.5000000000000000]]], device='cuda:0', dtype=torch.bfloat16,\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Don't use"
      ],
      "metadata": {
        "id": "7eSshYtM3ZA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_token_ids = []\n",
        "# Start with the combined embeddings and attention mask after injecting your vector\n",
        "current_embeddings = combined_embeddings\n",
        "current_attention_mask = combined_attention_mask\n",
        "current_sequence_length = combined_embeddings.shape[1]\n",
        "\n",
        "# Number of tokens to generate using the vector-based approach\n",
        "num_tokens_to_generate_with_vector = 5 # Example: generate 5 tokens\n",
        "\n",
        "for _ in range(num_tokens_to_generate_with_vector):\n",
        "    # Pass current embeddings through transformer layers\n",
        "    # We need to be careful about how past_key_values are handled if we want efficiency.\n",
        "    # For simplicity here, we re-process the entire sequence each time.\n",
        "    # For better performance, you would use the `past_key_values` returned by the layer forward pass.\n",
        "\n",
        "    # Let's do the inefficient full pass for demonstration:\n",
        "    current_hidden_states = current_embeddings\n",
        "    for layer in transformer_layers:\n",
        "         position_ids = torch.arange(0, current_sequence_length, dtype=torch.long, device='cuda').unsqueeze(0)\n",
        "\n",
        "         layer_output = layer(current_hidden_states,\n",
        "                              attention_mask=current_attention_mask,\n",
        "                              position_ids=position_ids)\n",
        "         current_hidden_states = layer_output[0]\n",
        "         # If we were using `use_cache=True`, layer_output would also contain `past_key_value`.\n",
        "\n",
        "    # Get logits for the last position\n",
        "    logits = model.lm_head(current_hidden_states[:, -1:, :]) # shape: [batch_size, 1, vocab_size]\n",
        "\n",
        "    # Sample the next token (e.g., greedy sampling or sampling with temperature/top-p)\n",
        "    next_token_id = torch.argmax(logits, dim=-1) # Greedy sampling example\n",
        "    generated_token_ids.append(next_token_id.item())\n",
        "\n",
        "    # Get the embedding of the sampled token\n",
        "    next_token_embedding = embeddings(next_token_id) # shape: [batch_size, 1, hidden_size]\n",
        "\n",
        "    # Concatenate the current embeddings with the new token embedding\n",
        "    current_embeddings = torch.cat((current_embeddings, next_token_embedding), dim=1)\n",
        "\n",
        "    # Extend the attention mask\n",
        "    next_mask_column = torch.ones((current_attention_mask.shape[0], 1), dtype=torch.long).to('cuda')\n",
        "    current_attention_mask = torch.cat((current_attention_mask, next_mask_column), dim=1)\n",
        "\n",
        "    # Update sequence length\n",
        "    current_sequence_length += 1\n",
        "\n",
        "print(\"\\nGenerated token IDs using vector injection:\")\n",
        "print(generated_token_ids)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(tokenizer.decode(generated_token_ids))\n",
        "\n",
        "# When you want to stop providing your vector and revert to normal generation,\n",
        "# you would simply continue generating using the standard `model.generate()` method,\n",
        "# providing the sequence generated so far (prompt + tokens generated from your vector)\n",
        "# as the input.\n",
        "\n",
        "# Example of continuing with standard generation:\n",
        "# full_generated_sequence = inputs[0].tolist() + generated_token_ids\n",
        "# full_input_tensor = torch.tensor([full_generated_sequence], dtype=torch.long).to('cuda')\n",
        "\n",
        "# # Generate more tokens normally\n",
        "# print(\"\\nContinuing with standard generation:\")\n",
        "# output_ids = model.generate(full_input_tensor,\n",
        "#                             max_length=len(full_generated_sequence) + 20, # Generate 20 more tokens\n",
        "#                             num_return_sequences=1)\n",
        "\n",
        "# print(\"\\nFull generated text:\")\n",
        "# print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        "\n",
        "# Note: This manual loop is less efficient than using the model's `generate` method\n",
        "# with `use_cache=True`, as it re-computes attention and feed-forward for the entire\n",
        "# sequence at each step. Implementing the `past_key_values` handling for efficiency\n",
        "# would make this code much more complex. For a practical application, integrating\n",
        "# this vector injection into a custom generation loop that uses `past_key_values`\n",
        "# would be necessary for speed.\n"
      ],
      "metadata": {
        "id": "H1fMCMb32JAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}