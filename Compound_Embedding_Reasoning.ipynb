{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNgSg7wOozWc5/twAjM8QLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "659d19d1b0d340d9b09d77b3c0ba09e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c73db0cb4d5d4e419c97c2a0ff2ed8ea",
              "IPY_MODEL_9e809864f52249a0b5af9fb1e7ad9f54",
              "IPY_MODEL_64191744727b401f91c1f5c46fac9d66"
            ],
            "layout": "IPY_MODEL_d9a82012a6fd4dcdac80b04474f688c0"
          }
        },
        "c73db0cb4d5d4e419c97c2a0ff2ed8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd5368a22aae4d59a611f0ee8209c5e1",
            "placeholder": "​",
            "style": "IPY_MODEL_c6674ea7e43a494cafe6ca93e998a31c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9e809864f52249a0b5af9fb1e7ad9f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cbd4c58e77944b3906cf4da06525c47",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c57d838647cd457687cb70d4d9bd1d86",
            "value": 2
          }
        },
        "64191744727b401f91c1f5c46fac9d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd8b9e255d00452a832b5e3a7faf266e",
            "placeholder": "​",
            "style": "IPY_MODEL_1f842d226b154cea8cedc615330eab8a",
            "value": " 2/2 [00:05&lt;00:00,  2.78s/it]"
          }
        },
        "d9a82012a6fd4dcdac80b04474f688c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd5368a22aae4d59a611f0ee8209c5e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6674ea7e43a494cafe6ca93e998a31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cbd4c58e77944b3906cf4da06525c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c57d838647cd457687cb70d4d9bd1d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd8b9e255d00452a832b5e3a7faf266e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f842d226b154cea8cedc615330eab8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gut-puncture/Compound_Embedding_Reasoning/blob/main/Compound_Embedding_Reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Y8s_rV8ElvbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Install the libraries we'll need.\n",
        "!pip -q install --upgrade \"transformers==4.41.2\" \"huggingface_hub>=0.23.0\" \"accelerate>=0.29.0\" sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "t63tE2sUE0Rs",
        "outputId": "40911354-2169-40c5-e310-bdfbb1379d21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "pwHgAhE_lyoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_start_tokens = \"### Reasoning:\\n\"\n",
        "reasoning_end_tokens = \"###\"\n",
        "answer_start_tokens = \"### Answer:\\n\""
      ],
      "metadata": {
        "id": "5hXbNZitQgbd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ],
      "metadata": {
        "id": "F-fxcxCafKHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def create_compound_vector(model_outputs, embeddings, model_dtype, compound_p=0.98):\n",
        "    \"\"\"\n",
        "    Creates a compound vector from top-p tokens, preserving information\n",
        "    about all tokens the model strongly considered.\n",
        "\n",
        "    Args:\n",
        "        model_outputs: Raw model forward pass outputs\n",
        "        embeddings: Token embedding layer from the model\n",
        "        model_dtype: The dtype of the model (to ensure consistency)\n",
        "        compound_p: Probability threshold for compound vector (default: 0.98)\n",
        "\n",
        "    Returns:\n",
        "        compound_vector: Weighted sum of top-p token embeddings\n",
        "    \"\"\"\n",
        "    # Get logits for the last (next) token position\n",
        "    logits = model_outputs.logits[:, -1, :]\n",
        "\n",
        "    # Sort tokens by probability (highest first)\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
        "\n",
        "    # Find tokens that make up top-p probability mass\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    top_p_mask = cumulative_probs <= compound_p\n",
        "\n",
        "    # Always include the top token, even if p is very small\n",
        "    top_p_mask[..., 0] = True\n",
        "\n",
        "    # Get the selected tokens and their information\n",
        "    selected_tokens = sorted_indices[top_p_mask]\n",
        "    selected_probs = sorted_probs[top_p_mask]\n",
        "    selected_logits = sorted_logits[top_p_mask]\n",
        "\n",
        "    # Get embeddings for selected tokens\n",
        "    selected_embeddings = embeddings(selected_tokens)\n",
        "\n",
        "    # Create probability weights (renormalized)\n",
        "    weights = torch.softmax(selected_logits, dim=-1)\n",
        "\n",
        "    # Create compound vector: weighted average of embeddings\n",
        "    compound_vector = torch.sum(\n",
        "        selected_embeddings * weights.unsqueeze(-1),\n",
        "        dim=0, keepdim=True\n",
        "    )\n",
        "\n",
        "    # Ensure correct dtype to match model\n",
        "    compound_vector = compound_vector.to(dtype=model_dtype)\n",
        "\n",
        "    return compound_vector.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "\n",
        "def sample_token_normally(model_outputs, sampling_p=0.8):\n",
        "    \"\"\"\n",
        "    Performs normal top-p sampling to select one token.\n",
        "\n",
        "    Args:\n",
        "        model_outputs: Raw model forward pass outputs\n",
        "        sampling_p: Probability threshold for sampling (default: 0.8)\n",
        "\n",
        "    Returns:\n",
        "        sampled_token_id: Single token ID selected via sampling\n",
        "    \"\"\"\n",
        "    logits = model_outputs.logits[:, -1, :]\n",
        "\n",
        "    # Sort and get top-p tokens for sampling\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
        "\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    top_p_mask = cumulative_probs <= sampling_p\n",
        "    top_p_mask[..., 0] = True  # Always include top token\n",
        "\n",
        "    # Sample from the top-p distribution\n",
        "    sampling_tokens = sorted_indices[top_p_mask]\n",
        "    sampling_probs = sorted_probs[top_p_mask]\n",
        "\n",
        "    # Multinomial sampling\n",
        "    sampled_index = torch.multinomial(sampling_probs, num_samples=1)\n",
        "    sampled_token = sampling_tokens[sampled_index]\n",
        "\n",
        "    return sampled_token\n",
        "\n",
        "\n",
        "def create_thinking_vector(compound_vector, sampled_token, embeddings, model_dtype, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Blends compound vector with normally sampled token to create thinking advancement vector.\n",
        "\n",
        "    Args:\n",
        "        compound_vector: Weighted combination of top-p token embeddings\n",
        "        sampled_token: Token ID from normal sampling\n",
        "        embeddings: Token embedding layer\n",
        "        model_dtype: The dtype of the model (to ensure consistency)\n",
        "        alpha: Blending weight (0=only sampled token, 1=only compound vector)\n",
        "\n",
        "    Returns:\n",
        "        thinking_vector: Blended vector for advanced reasoning\n",
        "    \"\"\"\n",
        "    # Get embedding of the sampled token\n",
        "    sampled_embedding = embeddings(sampled_token).unsqueeze(0)\n",
        "\n",
        "    # Ensure both tensors have the same dtype\n",
        "    sampled_embedding = sampled_embedding.to(dtype=model_dtype)\n",
        "    compound_vector = compound_vector.to(dtype=model_dtype)\n",
        "\n",
        "    # Blend the two representations\n",
        "    # (1-alpha) * sampled + alpha * compound\n",
        "    thinking_vector = (1 - alpha) * sampled_embedding + alpha * compound_vector\n",
        "\n",
        "    return thinking_vector\n",
        "\n",
        "\n",
        "def create_attention_mask(input_length, device, dtype):\n",
        "    \"\"\"\n",
        "    Creates proper attention masks for the extended sequence.\n",
        "    This consolidates the redundant mask creation in the original code.\n",
        "\n",
        "    Args:\n",
        "        input_length: Length of the input sequence + 1 (for thinking vector)\n",
        "        device: Device to create tensors on\n",
        "        dtype: Data type to match model's expectations\n",
        "\n",
        "    Returns:\n",
        "        attention_mask: Proper mask for transformer layers\n",
        "    \"\"\"\n",
        "    # Create causal mask (lower triangular matrix)\n",
        "    # This prevents tokens from attending to future positions\n",
        "    seq_len = input_length\n",
        "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device))\n",
        "\n",
        "    # Convert to the format expected by transformer layers\n",
        "    # Shape: [batch_size, num_heads, seq_len, seq_len]\n",
        "    attention_mask = torch.where(\n",
        "        causal_mask.unsqueeze(0).unsqueeze(0),\n",
        "        torch.zeros(1, dtype=dtype, device=device),\n",
        "        torch.full([], torch.finfo(dtype).min, device=device)\n",
        "    )\n",
        "\n",
        "    return attention_mask\n",
        "\n",
        "\n",
        "def inject_thinking_vector(model, input_embeddings, thinking_vector):\n",
        "    \"\"\"\n",
        "    Injects the thinking vector into the model's processing pipeline.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        input_embeddings: Original prompt embeddings\n",
        "        thinking_vector: The thinking advancement vector to inject\n",
        "\n",
        "    Returns:\n",
        "        logits: Output logits after processing with thinking vector\n",
        "    \"\"\"\n",
        "    # Get model's dtype for consistency\n",
        "    model_dtype = next(model.parameters()).dtype\n",
        "\n",
        "    # Ensure all tensors have the correct dtype\n",
        "    input_embeddings = input_embeddings.to(dtype=model_dtype)\n",
        "    thinking_vector = thinking_vector.to(dtype=model_dtype)\n",
        "\n",
        "    # Combine original embeddings with thinking vector\n",
        "    combined_embeddings = torch.cat([input_embeddings, thinking_vector], dim=1)\n",
        "\n",
        "    # Create proper attention mask\n",
        "    seq_length = combined_embeddings.shape[1]\n",
        "    attention_mask = create_attention_mask(seq_length, combined_embeddings.device, model_dtype)\n",
        "\n",
        "    # Create position IDs for the extended sequence\n",
        "    position_ids = torch.arange(0, seq_length, dtype=torch.long, device=combined_embeddings.device).unsqueeze(0)\n",
        "\n",
        "    # Pass through transformer layers\n",
        "    hidden_states = combined_embeddings\n",
        "\n",
        "    for layer in model.model.layers:\n",
        "        layer_output = layer(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids\n",
        "        )\n",
        "        hidden_states = layer_output[0]\n",
        "\n",
        "    # Get the output from the thinking vector position (last position)\n",
        "    thinking_output = hidden_states[:, -1:, :]\n",
        "\n",
        "    # Convert to logits\n",
        "    logits = model.lm_head(thinking_output)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jc8eZxAHfHOC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Loop"
      ],
      "metadata": {
        "id": "rbjVCOqUf1OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_thinking_loop():\n",
        "    \"\"\"\n",
        "    Full thinking-then-answer loop.\n",
        "    1.  Repeatedly inject “thinking-advance” vectors until the running\n",
        "        inverse-perplexity metric is high enough (≈ low perplexity).\n",
        "    2.  Append the delimiter tokens and let HF `generate()` finish the answer.\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  SET-UP                                                            #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    MODEL_DIR = \"/content/drive/MyDrive/phi3_3.8B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_DIR, torch_dtype=\"auto\", device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    model_dtype = next(model.parameters()).dtype\n",
        "    device      = next(model.parameters()).device\n",
        "    embeddings  = model.model.embed_tokens          # convenience handle\n",
        "\n",
        "    print(f\"Model dtype: {model_dtype}, Device: {device}\")\n",
        "\n",
        "    # prompt text ------------------------------------------------------- #\n",
        "    user_text  = \"What is Photosynthesis?\"\n",
        "    sys_prompt = \"You are a helpful assistant. Think deeply about any request.\"\n",
        "    prompt = (\n",
        "        f\"<|system|>\\n{sys_prompt}<|end|>\\n\"\n",
        "        f\"<|user|>\\n{user_text}<|end|>\\n\"\n",
        "        f\"<|assistant|>\\n### Reasoning:\\n\"\n",
        "    )\n",
        "\n",
        "    # encode once ------------------------------------------------------- #\n",
        "    input_ids        = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    current_ids      = input_ids.clone()                   # keep IDs in sync\n",
        "    current_embeds   = embeddings(current_ids).to(model_dtype)\n",
        "\n",
        "    perplexities = []\n",
        "    max_thinking_steps = 100\n",
        "    step = 0\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  “THINKING” LOOP                                                   #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    while step < max_thinking_steps:\n",
        "        step += 1\n",
        "        print(f\"\\n🧠  Thinking step {step} …\")\n",
        "\n",
        "        # forward pass on the *embedding sequence*\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs_embeds=current_embeds)\n",
        "\n",
        "        # --- build compound & thinking vectors ------------------------ #\n",
        "        compound_vec = create_compound_vector(outputs, embeddings, model_dtype, 0.98)\n",
        "        sampled_tok  = sample_token_normally(outputs, 0.80)             # tensor scalar\n",
        "        thinking_vec = create_thinking_vector(\n",
        "            compound_vec, sampled_tok, embeddings, model_dtype, alpha=0.25\n",
        "        )\n",
        "\n",
        "        # --- update sequences ---------------------------------------- #\n",
        "        current_embeds = torch.cat([current_embeds, thinking_vec], dim=1)\n",
        "        current_ids    = torch.cat([current_ids, sampled_tok.unsqueeze(0)], dim=1)\n",
        "\n",
        "        # --- inverse-perplexity proxy -------------------------------- #\n",
        "        last_logits   = outputs.logits[:, -1, :]\n",
        "        max_prob      = torch.softmax(last_logits, dim=-1).max()\n",
        "        inv_perplex   = 1.0 / max_prob.item()          #  ≈ perplexity\n",
        "        perplexities.append(inv_perplex)\n",
        "\n",
        "        print(f\"Inverse-perplexity: {inv_perplex:.3f}\")\n",
        "\n",
        "        if len(perplexities) >= 6:\n",
        "            recent_avg = sum(perplexities[-6:]) / 6\n",
        "            print(f\"Avg over last 6: {recent_avg:.3f}\")\n",
        "            if recent_avg < 1.35:\n",
        "                print(\"✅  Condition met – stop thinking.\")\n",
        "                break\n",
        "    else:\n",
        "        print(\"\\n⚠️  Hit maximum thinking steps – moving on anyway.\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  SWITCH TO NORMAL GENERATION                                       #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # append delimiters\n",
        "    reasoning_end_id  = tokenizer.convert_tokens_to_ids(\"###\")\n",
        "    answer_start_text = \"### Answer:\\n\"\n",
        "    answer_start_ids  = tokenizer(answer_start_text, add_special_tokens=False,\n",
        "                                  return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    current_ids = torch.cat(\n",
        "        [current_ids,\n",
        "         torch.tensor([[reasoning_end_id]], device=device),\n",
        "         answer_start_ids],\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    # let the model finish naturally\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=current_ids,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=1.0,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    full_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
        "    print(\"\\n📝  FINAL OUTPUT\\n\" + \"-\"*60 + \"\\n\")\n",
        "    print(full_text)\n",
        "\n",
        "\n",
        "\n",
        "# The main innovation: preserving model's \"thoughts\" rather than discarding them\n",
        "if __name__ == \"__main__\":\n",
        "    main_thinking_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "659d19d1b0d340d9b09d77b3c0ba09e3",
            "c73db0cb4d5d4e419c97c2a0ff2ed8ea",
            "9e809864f52249a0b5af9fb1e7ad9f54",
            "64191744727b401f91c1f5c46fac9d66",
            "d9a82012a6fd4dcdac80b04474f688c0",
            "cd5368a22aae4d59a611f0ee8209c5e1",
            "c6674ea7e43a494cafe6ca93e998a31c",
            "3cbd4c58e77944b3906cf4da06525c47",
            "c57d838647cd457687cb70d4d9bd1d86",
            "cd8b9e255d00452a832b5e3a7faf266e",
            "1f842d226b154cea8cedc615330eab8a"
          ]
        },
        "id": "7bCes9pLfdXE",
        "outputId": "24e53bd5-7051-4363-f18a-fe21c79353ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "659d19d1b0d340d9b09d77b3c0ba09e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dtype: torch.bfloat16, Device: cuda:0\n",
            "\n",
            "🧠  Thinking step 1 …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverse-perplexity: 1.574\n",
            "\n",
            "🧠  Thinking step 2 …\n",
            "Inverse-perplexity: 2.321\n",
            "\n",
            "🧠  Thinking step 3 …\n",
            "Inverse-perplexity: 6.370\n",
            "\n",
            "🧠  Thinking step 4 …\n",
            "Inverse-perplexity: 1.294\n",
            "\n",
            "🧠  Thinking step 5 …\n",
            "Inverse-perplexity: 1.057\n",
            "\n",
            "🧠  Thinking step 6 …\n",
            "Inverse-perplexity: 1.778\n",
            "Avg over last 6: 2.399\n",
            "\n",
            "🧠  Thinking step 7 …\n",
            "Inverse-perplexity: 3.908\n",
            "Avg over last 6: 2.788\n",
            "\n",
            "🧠  Thinking step 8 …\n",
            "Inverse-perplexity: 1.275\n",
            "Avg over last 6: 2.614\n",
            "\n",
            "🧠  Thinking step 9 …\n",
            "Inverse-perplexity: 1.000\n",
            "Avg over last 6: 1.719\n",
            "\n",
            "🧠  Thinking step 10 …\n",
            "Inverse-perplexity: 2.924\n",
            "Avg over last 6: 1.991\n",
            "\n",
            "🧠  Thinking step 11 …\n",
            "Inverse-perplexity: 1.000\n",
            "Avg over last 6: 1.981\n",
            "\n",
            "🧠  Thinking step 12 …\n",
            "Inverse-perplexity: 2.446\n",
            "Avg over last 6: 2.092\n",
            "\n",
            "🧠  Thinking step 13 …\n",
            "Inverse-perplexity: 1.381\n",
            "Avg over last 6: 1.671\n",
            "\n",
            "🧠  Thinking step 14 …\n",
            "Inverse-perplexity: 1.659\n",
            "Avg over last 6: 1.735\n",
            "\n",
            "🧠  Thinking step 15 …\n",
            "Inverse-perplexity: 1.820\n",
            "Avg over last 6: 1.872\n",
            "\n",
            "🧠  Thinking step 16 …\n",
            "Inverse-perplexity: 1.868\n",
            "Avg over last 6: 1.696\n",
            "\n",
            "🧠  Thinking step 17 …\n",
            "Inverse-perplexity: 1.260\n",
            "Avg over last 6: 1.739\n",
            "\n",
            "🧠  Thinking step 18 …\n",
            "Inverse-perplexity: 1.006\n",
            "Avg over last 6: 1.499\n",
            "\n",
            "🧠  Thinking step 19 …\n",
            "Inverse-perplexity: 1.174\n",
            "Avg over last 6: 1.464\n",
            "\n",
            "🧠  Thinking step 20 …\n",
            "Inverse-perplexity: 1.085\n",
            "Avg over last 6: 1.369\n",
            "\n",
            "🧠  Thinking step 21 …\n",
            "Inverse-perplexity: 6.410\n",
            "Avg over last 6: 2.134\n",
            "\n",
            "🧠  Thinking step 22 …\n",
            "Inverse-perplexity: 1.004\n",
            "Avg over last 6: 1.990\n",
            "\n",
            "🧠  Thinking step 23 …\n",
            "Inverse-perplexity: 1.001\n",
            "Avg over last 6: 1.947\n",
            "\n",
            "🧠  Thinking step 24 …\n",
            "Inverse-perplexity: 2.129\n",
            "Avg over last 6: 2.134\n",
            "\n",
            "🧠  Thinking step 25 …\n",
            "Inverse-perplexity: 5.604\n",
            "Avg over last 6: 2.872\n",
            "\n",
            "🧠  Thinking step 26 …\n",
            "Inverse-perplexity: 3.132\n",
            "Avg over last 6: 3.213\n",
            "\n",
            "🧠  Thinking step 27 …\n",
            "Inverse-perplexity: 2.923\n",
            "Avg over last 6: 2.632\n",
            "\n",
            "🧠  Thinking step 28 …\n",
            "Inverse-perplexity: 1.394\n",
            "Avg over last 6: 2.697\n",
            "\n",
            "🧠  Thinking step 29 …\n",
            "Inverse-perplexity: 1.000\n",
            "Avg over last 6: 2.697\n",
            "\n",
            "🧠  Thinking step 30 …\n",
            "Inverse-perplexity: 1.000\n",
            "Avg over last 6: 2.509\n",
            "\n",
            "🧠  Thinking step 31 …\n",
            "Inverse-perplexity: 1.000\n",
            "Avg over last 6: 1.742\n",
            "\n",
            "🧠  Thinking step 32 …\n",
            "Inverse-perplexity: 1.634\n",
            "Avg over last 6: 1.492\n",
            "\n",
            "🧠  Thinking step 33 …\n",
            "Inverse-perplexity: 1.003\n",
            "Avg over last 6: 1.172\n",
            "✅  Condition met – stop thinking.\n",
            "\n",
            "📝  FINAL OUTPUT\n",
            "------------------------------------------------------------\n",
            "\n",
            "<|system|> You are a helpful assistant. Think deeply about any request.<|end|><|user|> What is Photosynthesis?<|end|><|assistant|> ### Reasoning:\n",
            "\n",
            "- Recognize the user's inquiry is about a scientific concept.\n",
            "\n",
            "- Determine the subject matter: Photosynthesis.\n",
            "<unk> ### Answer:\n",
            "\n",
            "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy, usually from the sun, into chemical energy in the form of glucose. This process involves taking in carbon dioxide (CO2) from the air and water (H2O) from the soil, and using the energy of light to produce glucose (C6H12O6) and release oxygen (O2) as a byproduct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP0Fumk4g2OP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}