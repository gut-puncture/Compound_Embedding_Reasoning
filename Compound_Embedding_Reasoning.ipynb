{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXkJ0x6kHyjQ8kXkJXU3Or",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a37a7ced19e4dfeb93b906ba4e26430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9ef0139f886440697ede41f913fc53f",
              "IPY_MODEL_30c1bd1caafe4176a258f20602a61701",
              "IPY_MODEL_7aa9fadfe77a4b22bc0a54b09f58dcc3"
            ],
            "layout": "IPY_MODEL_3312f357c93d41519bb83a6875233426"
          }
        },
        "a9ef0139f886440697ede41f913fc53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5423c384191e4d1e9404edb75992cf1f",
            "placeholder": "​",
            "style": "IPY_MODEL_af3e22c95ccf45339cf31fdf8965af46",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "30c1bd1caafe4176a258f20602a61701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6423a0cb5f844012b5675e5929343bbe",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33877209bba945d587d235f1fe1280b1",
            "value": 2
          }
        },
        "7aa9fadfe77a4b22bc0a54b09f58dcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22aa4c2287534a92999badda3f78236d",
            "placeholder": "​",
            "style": "IPY_MODEL_8169c14810e14c798069d4c22ebdc8aa",
            "value": " 2/2 [02:21&lt;00:00, 66.13s/it]"
          }
        },
        "3312f357c93d41519bb83a6875233426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5423c384191e4d1e9404edb75992cf1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af3e22c95ccf45339cf31fdf8965af46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6423a0cb5f844012b5675e5929343bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33877209bba945d587d235f1fe1280b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22aa4c2287534a92999badda3f78236d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8169c14810e14c798069d4c22ebdc8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gut-puncture/Compound_Embedding_Reasoning/blob/main/Compound_Embedding_Reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beIuPWYPBST_",
        "outputId": "1d4902b0-235a-4f9c-ea93-ef9424d33e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1️⃣ Mount your Drive so Colab sees it as a local folder.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 2️⃣ Define where you want to store the model weights *permanently*.\n",
        "MODEL_DIR = \"/content/drive/MyDrive/phi3_3.8B\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Install the libraries we'll need.\n",
        "!pip install --upgrade \"transformers==4.41.2\" \"huggingface_hub>=0.23.0\" \"accelerate>=0.29.0\" sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "t63tE2sUE0Rs",
        "outputId": "1c2486ea-cbb7-4069-955d-96d8f544b78b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (0.31.2)\n",
            "Collecting huggingface_hub>=0.23.0\n",
            "  Downloading huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: accelerate>=0.29.0 in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting accelerate>=0.29.0\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0) (4.13.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.23.0)\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.29.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.29.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.29.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.29.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.29.0) (3.0.2)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.32.2-py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-xet, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, tokenizers, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.31.2\n",
            "    Uninstalling huggingface-hub-0.31.2:\n",
            "      Successfully uninstalled huggingface-hub-0.31.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.6.0\n",
            "    Uninstalling accelerate-1.6.0:\n",
            "      Successfully uninstalled accelerate-1.6.0\n",
            "Successfully installed accelerate-1.7.0 hf-xet-1.1.2 huggingface_hub-0.32.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.19.1 transformers-4.41.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "torch.set_printoptions(precision=16, sci_mode=False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    torch_dtype=\"auto\",                 # Uses float16 on GPU, float32 on CPU.\n",
        "    device_map=\"auto\"                   # transformers + accelerate decide the best device.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "8a37a7ced19e4dfeb93b906ba4e26430",
            "a9ef0139f886440697ede41f913fc53f",
            "30c1bd1caafe4176a258f20602a61701",
            "7aa9fadfe77a4b22bc0a54b09f58dcc3",
            "3312f357c93d41519bb83a6875233426",
            "5423c384191e4d1e9404edb75992cf1f",
            "af3e22c95ccf45339cf31fdf8965af46",
            "6423a0cb5f844012b5675e5929343bbe",
            "33877209bba945d587d235f1fe1280b1",
            "22aa4c2287534a92999badda3f78236d",
            "8169c14810e14c798069d4c22ebdc8aa"
          ]
        },
        "id": "ZuIh5qD0E6Gp",
        "outputId": "edcb4bfc-660b-429a-df7c-62fc363c8273"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a37a7ced19e4dfeb93b906ba4e26430"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_text = \"What is Photosynthesis?\" #will be populated by the eval questions"
      ],
      "metadata": {
        "id": "HsiYqyYTR8GX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_start_tokens = \"### Reasoning:\\n\"\n",
        "reasoning_end_tokens = \"###\"\n",
        "answer_start_tokens = \"### Answer:\\n\"\n",
        "sys_prompt = \"You are a helpful assistant to a human. You will think deeply about any user request and asnwer as smartly as possible.\"\n",
        "prompt = (\n",
        "  f\"<|system|>\\n{sys_prompt}<|end|>\\n\"\n",
        "  f\"<|user|>\\n{user_text}<|end|>\\n\"\n",
        "  f\"<|assistant|>\\n### Reasoning:\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "5hXbNZitQgbd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "collapsed": true,
        "id": "mhhzZhIbRw4t",
        "outputId": "9c180737-2c6e-48ad-9a40-ee2b2ce32b98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3c41a47d85bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3834\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3836\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3837\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3838\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         clean_up_tokenization_spaces = (\n",
            "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_logits, sorted_indices = torch.sort(outputs.logits[:,-1,:], descending=True) #sorting the logits so we can do top-p sampling\n",
        "sorted_probs = torch.softmax(sorted_logits, dim=-1) #converted sorted logits into sorted probs\n",
        "cumulative_probs = torch.cumsum(sorted_probs, dim=-1) #doing a cumulative sum of probs so we can identify when the top-p sampling cut-off is reached"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRKQcu2NSHd5",
        "outputId": "0a72674d-4eb4-4e07-a8e6-406afe2ed8c4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.8912e-10, 3.7806e-08, 2.1329e-09,  ..., 2.4974e-12, 2.4974e-12,\n",
              "         2.4974e-12]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling only those tokens which have a combined probs of p\n",
        "p = 0.98\n",
        "selected_token_indices = []\n",
        "\n",
        "for token in range(len(sorted_indices.tolist()[0])):\n",
        "  if cumulative_probs.tolist()[0][token] < p:\n",
        "    selected_token_indices.append(sorted_indices.tolist()[0][token]) #token indices are actually token ids as well\n",
        "  else:\n",
        "    break\n",
        "print(selected_token_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7JaHHvZc4EA",
        "outputId": "d0f20563-984d-499c-b09d-25cf47276fcc"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13, 1762, 29896, 29899, 1576, 4819, 29902, 797]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_token_probs = sorted_probs[:,:len(selected_token_indices)].tolist()[0] #selecting the token probs for the selected token ids\n",
        "selected_token_logits = sorted_logits[:,:len(selected_token_indices)].tolist()[0] #selecting the token logits for the selected token ids"
      ],
      "metadata": {
        "id": "5GhCYVUGUWSq"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting embeddings of the selected tokens\n",
        "\n",
        "embeddings = model.model.embed_tokens #method to get token embeddings\n",
        "selected_token_indices_tensor = torch.tensor(selected_token_indices, dtype=torch.long).to('cuda') #converted list to tensor\n",
        "selected_token_embeddings = embeddings(selected_token_indices_tensor)"
      ],
      "metadata": {
        "id": "G7zuOzkhr1Z8"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#renormalising probs\n",
        "\n",
        "selected_token_renormalised_probs = torch.softmax(torch.tensor(selected_token_logits), dim=-1)"
      ],
      "metadata": {
        "id": "drkd5hlH9oI_"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_token_renormalised_probs.dtype)\n",
        "print(selected_token_embeddings.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afecy8NYB7HS",
        "outputId": "38bf146b-11e2-4dde-a31e-a41ac0f6688d"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_token_renormalised_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmCk6pMDCQOo",
        "outputId": "4907598a-2d70-4a7e-8f3f-b47c574a2fa2"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6419127583503723, 0.1839110851287842, 0.0868734419345856,\n",
              "        0.0410361066460609, 0.0193840861320496, 0.0117570422589779,\n",
              "        0.0117570422589779, 0.0033684489317238], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_token_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ThVmuj2CWqV",
        "outputId": "57779081-8669-4e31-b2f9-8faa0a384a8c"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0029144287109375,  0.0043334960937500, -0.0016937255859375,\n",
              "          ...,  0.0112304687500000,  0.1025390625000000,\n",
              "         -0.0136108398437500],\n",
              "        [-0.0273437500000000, -0.0032501220703125,  0.0208740234375000,\n",
              "          ...,  0.0266113281250000,  0.0153808593750000,\n",
              "          0.0197753906250000],\n",
              "        [ 0.0290527343750000, -0.0227050781250000, -0.0179443359375000,\n",
              "          ..., -0.0045471191406250, -0.0898437500000000,\n",
              "         -0.0132446289062500],\n",
              "        ...,\n",
              "        [-0.0598144531250000, -0.0527343750000000,  0.0429687500000000,\n",
              "          ..., -0.0388183593750000, -0.0747070312500000,\n",
              "          0.0815429687500000],\n",
              "        [ 0.0022888183593750,  0.0036010742187500, -0.0252685546875000,\n",
              "          ...,  0.0610351562500000, -0.0035400390625000,\n",
              "          0.0088500976562500],\n",
              "        [ 0.0158691406250000, -0.0186767578125000, -0.0071716308593750,\n",
              "          ...,  0.0034637451171875, -0.0742187500000000,\n",
              "         -0.0240478515625000]], device='cuda:0', grad_fn=<ToCopyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_token_embeddings * (selected_token_renormalised_probs).unsqueeze(-1).to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nUos1jx-Yzk",
        "outputId": "930c7555-cd94-4f32-b6a9-86699702bd50"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    -0.0018708090065047,      0.0027817264199257,\n",
              "             -0.0010872241109610,  ...,\n",
              "              0.0072089810855687,      0.0658211335539818,\n",
              "             -0.0087369717657566],\n",
              "        [    -0.0050288187339902,     -0.0005977334803902,\n",
              "              0.0038389642722905,  ...,\n",
              "              0.0048941182903945,      0.0028287104796618,\n",
              "              0.0036369136068970],\n",
              "        [     0.0025239109527320,     -0.0019724683370441,\n",
              "             -0.0015588862588629,  ...,\n",
              "             -0.0003950239042751,     -0.0078050359152257,\n",
              "             -0.0011506065493450],\n",
              "        ...,\n",
              "        [    -0.0007032410358079,     -0.0006200002972037,\n",
              "              0.0005051853950135,  ...,\n",
              "             -0.0004563890979625,     -0.0008783336961642,\n",
              "              0.0009587041568011],\n",
              "        [     0.0000269097345154,      0.0000423379824497,\n",
              "             -0.0002970834611915,  ...,\n",
              "              0.0007175928913057,     -0.0000416203874920,\n",
              "              0.0001040509741870],\n",
              "        [     0.0000534543905815,     -0.0000629117057542,\n",
              "             -0.0000241572724917,  ...,\n",
              "              0.0000116674482342,     -0.0002500020782463,\n",
              "             -0.0000810039564385]], device='cuda:0', grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.6419127583503723*-0.0029144287109375"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v1dPdqJ_tJg",
        "outputId": "b0a0f66c-0852-4105-8db0-36881bea481b"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0018708089728534105"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: bro I manually multiplied the numbers to check selected_token_embeddings * (selected_token_renormalised_probs).unsqueeze(-1).to('cuda') and I got -0.0018708089728534105 instead of -0.0018708090065047. what's the issue?\n",
        "# The issue seems to be the difference in dtype. Can we make both the tensors the same datatype with the largest precision. The two data types are torch.bfloat16(selected_token_embeddings) and torch.float32(selected_token_renormalised_probs).\n",
        "\n",
        "selected_token_renormalised_probs = torch.softmax(torch.tensor(selected_token_logits, dtype=torch.float32), dim=-1).to('cuda')\n",
        "selected_token_embeddings = selected_token_embeddings.to(torch.float32)\n",
        "print(selected_token_embeddings * (selected_token_renormalised_probs).unsqueeze(-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utXLKRmOAlvb",
        "outputId": "a9111fc1-46e4-4ec4-df4c-1192b335ceca"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    -0.0018708090065047,      0.0027817264199257,\n",
            "             -0.0010872241109610,  ...,\n",
            "              0.0072089810855687,      0.0658211335539818,\n",
            "             -0.0087369717657566],\n",
            "        [    -0.0050288187339902,     -0.0005977334803902,\n",
            "              0.0038389642722905,  ...,\n",
            "              0.0048941182903945,      0.0028287104796618,\n",
            "              0.0036369136068970],\n",
            "        [     0.0025239109527320,     -0.0019724683370441,\n",
            "             -0.0015588862588629,  ...,\n",
            "             -0.0003950239042751,     -0.0078050359152257,\n",
            "             -0.0011506065493450],\n",
            "        ...,\n",
            "        [    -0.0007032410358079,     -0.0006200002972037,\n",
            "              0.0005051853950135,  ...,\n",
            "             -0.0004563890979625,     -0.0008783336961642,\n",
            "              0.0009587041568011],\n",
            "        [     0.0000269097345154,      0.0000423379824497,\n",
            "             -0.0002970834611915,  ...,\n",
            "              0.0007175928913057,     -0.0000416203874920,\n",
            "              0.0001040509741870],\n",
            "        [     0.0000534543905815,     -0.0000629117057542,\n",
            "             -0.0000241572724917,  ...,\n",
            "              0.0000116674482342,     -0.0002500020782463,\n",
            "             -0.0000810039564385]], device='cuda:0', grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_token_renormalised_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjQEzJvY_n8q",
        "outputId": "afbd5b4b-ea0a-4270-c09e-4f7e977298cd"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6419127583503723, 0.1839110851287842, 0.0868734419345856,\n",
              "        0.0410361066460609, 0.0193840861320496, 0.0117570422589779,\n",
              "        0.0117570422589779, 0.0033684489317238])"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "select_token_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEZ00zdS_q2m",
        "outputId": "83e0ad4f-fb9c-4c2a-adb2-62bd5264200a"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0029144287109375,  0.0043334960937500, -0.0016937255859375,\n",
              "          ...,  0.0112304687500000,  0.1025390625000000,\n",
              "         -0.0136108398437500],\n",
              "        [-0.0273437500000000, -0.0032501220703125,  0.0208740234375000,\n",
              "          ...,  0.0266113281250000,  0.0153808593750000,\n",
              "          0.0197753906250000],\n",
              "        [ 0.0290527343750000, -0.0227050781250000, -0.0179443359375000,\n",
              "          ..., -0.0045471191406250, -0.0898437500000000,\n",
              "         -0.0132446289062500],\n",
              "        ...,\n",
              "        [-0.0598144531250000, -0.0527343750000000,  0.0429687500000000,\n",
              "          ..., -0.0388183593750000, -0.0747070312500000,\n",
              "          0.0815429687500000],\n",
              "        [ 0.0022888183593750,  0.0036010742187500, -0.0252685546875000,\n",
              "          ...,  0.0610351562500000, -0.0035400390625000,\n",
              "          0.0088500976562500],\n",
              "        [ 0.0158691406250000, -0.0186767578125000, -0.0071716308593750,\n",
              "          ...,  0.0034637451171875, -0.0742187500000000,\n",
              "         -0.0240478515625000]], device='cuda:0', dtype=torch.bfloat16,\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multiplying vector embeddings for the selected tokens and multiplying them with the corresponding probs\n",
        "\n",
        "multiplied_embeddings = selected_token_embeddings * torch.tensor(selected_token_renormalised_probs).unsqueeze(-1).to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_HdbJhNs4lq",
        "outputId": "c5b3a27c-90cf-4d07-d810-bd7b62d1d409"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-150-c991eb93015b>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  multiplied_embeddings = selected_token_embeddings * torch.tensor(selected_token_renormalised_probs).unsqueeze(-1).to('cuda')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmyNvatI_c5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4eVm-ki_c2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "s4AOd36es78x"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I have the embeddings of tokens in the select_token_embeddings tensor and I want the embeddings to be multiplied by the corresponding number in the selected_token_probs list.\n",
        "\n"
      ],
      "metadata": {
        "id": "dPgDT1n4yj1C"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_token_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xltFIZ4m09Us",
        "outputId": "d7b9b2dc-624b-4455-b591-9418c77f155a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to do this: torch.tensor(selected_token_probs).unsqueeze(-1).to('cuda'), but I need all the decimal points present in the original list to be present in the tensor.\n",
        "# when I apply dtype=torch.float32 I get the value 0.6276 in the first element even though it is 0.6276402473449707 in the original list.\n",
        "\n",
        "# Use dtype=torch.float64 to preserve more precision\n",
        "multiplied_embeddings = selected_token_embeddings * torch.tensor(selected_token_probs, dtype=torch.float64).unsqueeze(-1).to('cuda')"
      ],
      "metadata": {
        "id": "tZxfK2V21XGe"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "multiplied_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0nSRD9k0pB_",
        "outputId": "b669fe4d-b17b-43bb-d9d9-e64367410805"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    -0.0018292127570021,      0.0027198765601497,\n",
              "             -0.0010630503456923,  ...,\n",
              "              0.0070486941840500,      0.0643576425500214,\n",
              "             -0.0085427108861040],\n",
              "        [    -0.0049170065321960,     -0.0005844432987487,\n",
              "              0.0037536076652032,  ...,\n",
              "              0.0047853010000836,      0.0027658161743602,\n",
              "              0.0035560493670346],\n",
              "        [     0.0024677936289663,     -0.0019286118276796,\n",
              "             -0.0015242254767145,  ...,\n",
              "             -0.0003862408095756,     -0.0076314962643664,\n",
              "             -0.0011250235661464],\n",
              "        ...,\n",
              "        [    -0.0006876050042592,     -0.0006062150241632,\n",
              "              0.0004939529826515,  ...,\n",
              "             -0.0004462416150091,     -0.0008588046175646,\n",
              "              0.0009373880466228],\n",
              "        [     0.0000263114159793,      0.0000413966278074,\n",
              "             -0.0002904780324116,  ...,\n",
              "              0.0007016377594482,     -0.0000406949900480,\n",
              "              0.0001017374751200],\n",
              "        [     0.0000522658689306,     -0.0000615129072798,\n",
              "             -0.0000236201523052,  ...,\n",
              "              0.0000114080310070,     -0.0002444434485369,\n",
              "             -0.0000792028936871]], device='cuda:0', dtype=torch.float64,\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClwHuFeRzGlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}